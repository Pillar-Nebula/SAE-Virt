CASTELLI - MASNY
## Projet de Migration : Sortir de VMware


### SYNTH√àSE : Sc√©nario de Sortie de VMware

Nous avons choisi de  s√©lectionner Proxmox et HyperV
#### Verdict : Choix de Proxmox VE

Notre analyse technique et financi√®re d√©signe **Proxmox VE** comme la meilleure solution de remplacement.

### 4 Arguments D√©cisifs

1.  Rentabilit√© Imm√©diate
Proxmox permet une √©conomie de 40 000 ‚Ç¨ √† l'achat pour 3 serveurs par rapport √† Hyper-V (Licences Datacenter + Veeam). Il reste plus rentable jusqu'√† 58 VMs.

2.  Migration Fluide (VMware)
Proxmox, depuis quelques versions, int√®gre un assistant d'import qui aspire les VMs VMware en quelques clics. √Ä l'inverse, Hyper-V impose une conversion de format peu arrangeant.

3.  Rapidit√© de Mise en ≈íuvre
L'installation de Proxmox est "cl√© en main" (Cluster + Ceph + R√©seau op√©rationnels en 4h). Hyper-V (S2D dans notre cas) est quand m√™me plus compliqu√©, n√©cessitant une configuration assez complexe.

4.  Solution "Tout-en-un"
Proxmox inclut nativement le Stockage Distribu√© (Ceph) et le Backup (PBS). Hyper-V n√©cessite des r√¥les en plus ou des logiciels tiers payants.

### Visualisation Strat√©gique

<img src="images/Pasted%20image%2020251219200028.png" width="300">

Conclusion :
Bien que Microsoft Hyper-V soit une solution assez viable pour les entreprises d√©j√† "Full Microsoft", Proxmox VE surpasse son concurrent sur les crit√®res de co√ªt, de simplicit√© et d'outillage de migration, r√©pondant parfaitement √† la probl√©matique de remplacement de VMware.

---

# PARTIE 1 : DOSSIER D'ARCHITECTURE TECHNIQUE (PROXMOX VE)
**Responsable :** Alexandre

## 1. Pr√©sentation de l'Architecture

Cette partie d√©crit la techniques de l'infrastructure de virtualisation mise en place. L'architecture repose sur le principe de **Virtualisation Imbriqu√©e (Nested)** pour simuler un cluster de production √† trois n≈ìuds sur un serveur physique unique.

### 1.1 Mat√©riel Physique (Niveau 0)

L'h√¥te physique est un serveur Dell PowerEdge R640 :

* **CPU :** AMD EPYC (Instructions SVM activ√©es).
* **RAM :** Dimensionn√©e pour supporter 3 hyperviseurs virtuels.
* **Stockage :** 2x SSD 1.7 To configur√©s en **RAID 1 Mat√©riel** (Contr√¥leur PERC H730P).
* **R√©seau :** Interface `eno1` (1 Gbps) connect√©e au switch de salle.

### 1.2 Architecture Logique (Niveau 1)

Le cluster Proxmox (`SAE-Cluster`) est compos√© de trois n≈ìuds virtuels interconnect√©s via le protocole Corosync.

| N≈ìud | Hostname | IP Management | R√¥le |
| :--- | :--- | :--- | :--- |
| **Node 1** | `pve1` | `10.202.6.220` | Calcul + Stockage (Ceph/ZFS) |
| **Node 2** | `pve2` | `10.202.6.221` | Calcul + Stockage (Ceph/ZFS) |
| **Node 3** | `pve3` | `10.202.6.222` | Calcul + Stockage (Ceph/ZFS) |

---

## 2. Configuration de l'Hyperviseur

### 2.1 Virtualisation Imbriqu√©e
Pour permettre l'ex√©cution de machines virtuelles (KVM) √† l'int√©rieur des n≈ìuds virtuels, le mode CPU "Host" a √©t√© forc√© dans la configuration de l'hyperviseur racine.
### 2.2 Configuration R√©seau

Le r√©seau repose sur un pont Linux standard (`vmbr0`) sans VLAN tagu√© au niveau de l'h√¥te, la segmentation √©tant g√©r√©e en amont.

**Configuration r√©seau type (`/etc/network/interfaces`) :**

```
auto lo
iface lo inet loopback

auto eno1
iface eno1 inet manual

auto vmbr0
iface vmbr0 inet static
    address 10.202.6.220/16 < ici par exemple c'est le node 1
    gateway 10.202.6.254
    bridge-ports eno1
    bridge-stp off
    bridge-fd 0
```
#### 2.3 Optimisation des Flux Cluster (Corosync)

Le bon fonctionnement d'un cluster Proxmox repose sur **Corosync**, le moteur de communication inter-n≈ìuds.

- **Contrainte critique :** Corosync exige une latence r√©seau inf√©rieure √† 2ms.
- **Probl√©matique Nested :** Dans notre environnement virtualis√©, le r√©seau est partag√©.
- **Configuration Kronosnet (knet) :** Proxmox 8 utilise par d√©faut le transport _Kronosnet_ en Unicast. Cela nous √©vite la complexit√© de configuration du Multicast (IGMP Snooping) sur les switchs physiques Dell, tout en garantissant le chiffrement des √©changes de contr√¥le entre les n≈ìuds `pve1`, `pve2` et `pve3`.
---

## 3. Strat√©gie de Stockage Hybride

L'infrastructure utilise une approche hybride combinant stockage distribu√©, local et r√©pliqu√©.

### 3.1 Stockage Distribu√© : Ceph RBD

Utilis√© pour la Haute Disponibilit√© (VMs Web & DNS).

- **Version :** Ceph Reef (18.2).
- **Topologie :** 3 Moniteurs (MON), 3 Managers (MGR), 3 OSDs.
- **Pool :** `pool-vms` (Size: 3, Min_size: 2).
- **Volume :** Chaque n≈ìud contribue avec un vDisk de 100 Go (`/dev/sdb`).

**√âtat du Cluster :**

![Capture d'√©cran](images/Pasted%20image%2020251219213433.png)

#### 3.1.1 Fonctionnement de l'algorithme CRUSH

Contrairement √† un RAID classique qui utilise une table d'allocation centralis√©e (goulot d'√©tranglement), Ceph utilise l'algorithme **CRUSH** (Controlled Replication Under Scalable Hashing).

- **Distribution :** Lorsqu'une VM √©crit une donn√©e, le client calcule √† la vol√©e sur quel OSD (Disque) la donn√©e doit aller.
- **R√©plication Synchrone :** Avec un `size=3` et `min_size=2`, l'√©criture n'est confirm√©e √† la VM que lorsque la donn√©e a √©t√© √©crite physiquement sur **au moins 2 n≈ìuds**. Cela garantit qu'aucune perte de donn√©es n'est possible m√™me en cas de coupure √©lectrique imm√©diate apr√®s une √©criture.
- **Backend BlueStore :** Les OSD ont √©t√© format√©s avec le backend _BlueStore_, qui permet √† Ceph d'√©crire directement sur le disque brut (raw device) sans passer par une couche de syst√®me de fichiers (ext4/xfs), augmentant ainsi les IOPS.
### 3.2 Stockage Local : ZFS

Utilis√© pour les performances I/O (VM Client).

- **Pool :** `zfs-local`
- **Configuration :** Single Disk (`/dev/sdc`) avec compression LZ4 activ√©e.
#### 3.2.1 Gestion de la m√©moire et Compression (ARC)

L'utilisation de ZFS implique une consommation m√©moire sp√©cifique li√©e √† l'**ARC (Adaptive Replacement Cache)**.

- **Strat√©gie :** ZFS utilise la RAM disponible pour mettre en cache les donn√©es les plus fr√©quemment lues (Read Cache). C'est pourquoi nos n≈ìuds virtuels ont √©t√© dimensionn√©s avec 32 Go de RAM, afin de laisser environ 4 Go √† l'ARC pour acc√©l√©rer les VMs.
- **Compression LZ4 :** J'ai activ√© la compression LZ4 sur le pool. C'est un algorithme "zero-cost" : la vitesse de compression du CPU est plus rapide que la vitesse d'√©criture du disque. Cela nous permet de gagner environ **1.4x** d'espace disque tout en augmentant virtuellement la vitesse d'√©criture.
### 3.3 Stockage de Sauvegarde : ZFS Replication

Utilis√© pour le PRA du serveur de backup.

- **Source :** `pve3/zfs-repl`
- **Cible :** `pve2/zfs-repl`
- **Fr√©quence :** Synchronisation toutes les 15 minutes.

---

## 4. Services D√©ploy√©s

### 4.1 Serveur DNS (VM 105)

- **OS :** Alpine Linux.
- **Logiciel :** ISC Bind9.
- **Zone :** `sae.lan`
### 4.2 Serveur Web (VM 106)

- **OS :** Alpine Linux
- **Logiciel :** Nginx  
- **Contenu :** Page statique HTML accessible via `www.sae.lan`    
### 4.3 Proxmox Backup Server (VM 110)

- **IP :** `10.202.6.255`.
- **R√¥le :** Centralisation des sauvegardes d√©dupliqu√©es.
- **Datastore :** `backup-store` (sur ZFS).    

### 4.4 Proxmox Datacenter Manager

- **IP :** `10.202.6.255`
    
- **R√¥le :** Supervision centralis√©e et interface de gestion globale du cluster.
    
- **Fonction :** Monitoring en temps r√©el de la charge et des n≈ìuds.
---

## 5. M√©canismes de R√©silience

### 5.1 Haute Disponibilit√© (HA)

Le gestionnaire de ressources  de Proxmox est configur√© pour surveiller les VMs critiques.

- **Groupe HA :** `critical-services`
- Tentative de migration, sinon red√©marrage.
- **Membres :** VM 105 (DNS), VM 106 (Web).
#### 5.1.1 Gestion du Quorum et Watchdog

La stabilit√© du cluster repose sur le concept de **Quorum** (Majorit√© de votes).

- **Vote :** Nous avons 3 n≈ìuds, donc 3 votes. La majorit√© requise est de 2.
- **Sc√©nario de Panne :** Si le N≈ìud 1 tombe, les N≈ìuds 2 et 3 conservent le Quorum (2 votes > 1.5). Ils d√©cident alors de "tuer" (Fence) le N≈ìud 1 pour √©viter qu'il ne corrompe les donn√©es, puis red√©marrent ses services.
- **Protection Watchdog :** Un m√©canisme de "Watchdog" mat√©riel (√©mul√© ici par le noyau Linux `softdog`) est actif sur chaque n≈ìud. Si le n≈ìud perd le contact avec le cluster pendant plus de 60 secondes, le Watchdog force un red√©marrage brutal du serveur (Hard Reset) pour garantir l'int√©grit√© du syst√®me (Self-Fencing).

### 5.2 Plan de Reprise d'Activit√©

En cas de perte totale du n≈ìud h√©bergeant le PBS, la r√©plication ZFS permet le red√©marrage du service de sauvegarde sur le n≈ìud voisin sans restauration longue, garantissant l'acc√®s aux archives de backup.

---

# PARTIE 2 : DOSSIER TECHNIQUE - HYPER-V (Romain)

**Responsable :** Romain

## 1. Introduction et Architecture Globale

Cette partie d√©taille la m√©thodologie de mise en ≈ìuvre d'une infrastructure de virtualisation √† haute disponibilit√© (HA).

Pour simuler un environnement avec plusieurs n≈ìuds avec notre mat√©riel, nous avons d√©ploy√© une architecture en **Virtualisation Imbriqu√©e (Nested Virtualization)** :

- **Niveau 0 (Physique) :** Serveur Dell ex√©cutant Windows Server Datacenter.
- **Niveau 1 (Logique) :** Cluster de 3 machines virtuelles agissant comme n≈ìuds.
- **Stockage :** Agr√©gation logicielle des disques via la technologie **Storage Spaces Direct (S2D)**.

---

## 2. Pr√©paration de l'Infrastructure Physique (Niveau 0)

### 2.1 Mat√©riel

L'infrastructure repose sur un serveur Dell (Switch 6). La premi√®re √©tape a consist√© √† faciliter l'administration via la carte de gestion √† distance (iDRAC).

- **Adressage iDRAC :** `10.202.6.216 /16`
- **Passerelle :** `10.202.255.254`

L'acc√®s iDRAC nous a permis de piloter l'alimentation, de monter les images ISO virtuellement et d'acc√©der √† la console distante pour l'installation de l'OS.

### 2.2 Stockage Physique

Pour garantir le bon fonctionnement du futur S2D, il √©tait important de pr√©senter des disques vierges. Nous avons proc√©d√© √† un nettoyage avant l'installation de l'OS, convertissant les disques au format GPT.

---

## 3. Configuration de l'Hyperviseur H√¥te

### 3.1 Syst√®me d'Exploitation et Licences

Le choix s'est port√© sur Windows Server √©dition Datacenter.

Contrairement √† l'√©dition Standard, la version Datacenter est un pr√©requis pour l'activation de la fonctionnalit√© S2D et permet une virtualisation illimit√©e.

### 3.2 Activation du "MAC Address Spoofing"

C'est un point de configuration assez important pour le Nested Clustering. Par d√©faut, un port de vSwitch Hyper-V n'accepte que les trames provenant de l'adresse MAC de la VM connect√©e.

Or ici, les VMs imbriqu√©es g√©n√®rent des trames avec leurs propres adresses MAC.

Nous avons activ√© l'usurpation d'adresse MAC sur les cartes r√©seaux virtuelles des 3 n≈ìuds pour autoriser le transit de ces paquets :

PowerShell

```
Get-VMNetworkAdapter -VMName "Node*" | Set-VMNetworkAdapter -MacAddressSpoofing On
```

Sans cette commande, aucune communication r√©seau vers les VMs finales n'aurait √©t√© possible.

---

## 4. Impl√©mentation du Cluster S2D (C≈ìur du syst√®me)

### 4.1 Storage Spaces Direct (S2D)

S2D permet de cr√©er un stockage hautement disponible en utilisant des disques locaux attach√©s √† chaque serveur, √©liminant le besoin d'une baie SAN physique.

#### 4.1.1 Activation

Nous avons activ√© S2D (`Enable-ClusterS2D`) pour qu'il prenne le contr√¥le des disques, permettant √† chaque n≈ìud de lire et √©crire sur les disques des voisins via le r√©seau.

#### 4.1.2 Strat√©gie de R√©silience : Miroir √† 3 Voies

Nous avons configur√© le pool de stockage en **Miroir √† 3 voies**.

- **Fonctionnement :** Chaque bloc de donn√©e est √©crit simultan√©ment sur 3 disques situ√©s sur 3 n≈ìuds diff√©rents.
- Cette m√©thode offre la r√©silience maximale. Elle permet de tol√©rer la panne simultan√©e de **2 n≈ìuds** (ou 2 disques) tout en garantissant l'int√©grit√© des donn√©es.

#### 4.1.3 Syst√®me de Fichiers ReFS

Les volumes partag√©s de cluster ont √©t√© format√©s en ReFS (Resilient File System).

Avantages :

1. D√©tection et correction automatique de la corruption de donn√©es.
2. Optimis√© pour la virtualisation et est donc plus performant.

#### 4.1.4 Gestion des vDisk

Lors de la cr√©ation du volume vDisk_S2D (30 Go pour accueillir la VM), nous avons laiss√© 50 Go d'espace non allou√© dans ce m√™me pool.

Cet espace de r√©serve permet au syst√®me de lancer une reconstruction automatique en cas de perte d'un disque.

---

## 5. D√©ploiement des Services et Validation

### 5.1 Tests de R√©silience et R√©sultats

Une s√©rie de tests a valid√© le bon fonctionnement de la solution :

|**Test Effectu√©**|**M√©thodologie Technique**|**R√©sultat**|
|---|---|---|
|**Live Migration**|D√©placement d'une VM active entre N≈ìud 1 et N≈ìud 2 via `Move-ClusterVirtualMachineRole`.|**Succ√®s :** 0 perte de ping. Continuit√© de service assur√©e.|
|**Failover (HA)**|Simulation d'une panne mat√©rielle du N≈ìud h√©bergeant le service Web.|**Succ√®s :** Le cluster a d√©tect√© la perte de signal et a red√©marr√© la VM sur un autre n≈ìud.|
|**Int√©grit√© Stockage**|Simulation de d√©connexion d'un disque physique du pool S2D.|**Succ√®s :** Le volume est rest√© "Online" en mode d√©grad√©. Les VMs n'ont subi aucune interruption.|

---

# PARTIE 3 : COMPTE RENDU D'ACTIVIT√â JOURNALIER (Commun)

Cette partie rend compte de l'avancement chronologique des travaux.

---

## Jour 1 : Pr√©paration et Premiers D√©ploiements

### 1. R√©organisation de la Baie (Commun)

Avant toute chose, nous avons, avec l‚Äôaide d‚Äôautres groupes (notamment celui de Valentin, de Pierre et de Soyfoudine), r√©organis√© les baies de la salle pour avoir un **c√¢blage propre** et identifier clairement les branchements de chaque groupe.

![Capture d'√©cran](images/Pasted%20image%2020251219200311.png)

_Incident :_ Durant cette phase, une erreur de c√¢blage a provoqu√© une boucle r√©seau, g√©n√©rant une temp√™te de broadcast d'environ **650 Go** sur le VLAN. L'incident a √©t√© identifi√© et corrig√© gr√¢ce √† l'intervention de Maxine.

Une fois le c√¢blage fini, chaque groupe a pu connecter les cartes iDRAC de ses serveurs pour travailler √† distance.

### 2. Configuration d‚ÄôiDRAC et Adressage (Commun)

Nous avons configur√© les cartes iDRAC de nos 2 serveurs (Switch 6 et 7).

Apr√®s analyse du plan d'adressage global (10.202.0.0/16), nous avons attribu√© une plage d'adresse √† chaque groupe, et nous avons utilis√© la plage 10.202.6.0/16.

|**Param√®tre**|**Serveur Hyper-V (Romain)**|**Serveur Proxmox (Alexandre)**|
|---|---|---|
|**IP iDRAC**|`10.202.6.216`|`10.202.6.17`|
|**Masque**|`255.255.0.0`|`255.255.0.0`|
|**Passerelle**|`10.202.255.254`|`10.202.255.254`|

#### Test de connexion

Nous nous sommes connect√©s via l'interface WEB iDRAC.

![Capture d'√©cran](images/Pasted%20image%2020251219200334.png)

### 3. Installation de l'Hyperviseur Proxmox (Alexandre)

Ici, je me suis occup√© du 7eme serveur en partant du haut.

J'ai √©tabli un premier cahier des charges : un hyperviseur principal (Bare Metal) h√©bergeant une infrastructure virtualis√©e (Nested) pour simuler un cluster, ce qui signifiait abandonner un des deux disques de 1TO7 qui m'√©taient attribu√©s. Je me suis rendu sur la console en ligne de l'idrac et j'ai charg√© un m√©dia virtuel, j'ai charg√© l'ISO de proxmox, j'ai ensuite red√©marr√© le serveur et j'ai pu configurer mon proxmox.

1. **Installation :** Boot sur l'ISO Proxmox.
2. **Stockage :** Installation standard sur les disques disponibles (le RAID mat√©riel n'√©tait pas encore configur√© √† ce stade du projet).

![Capture d'√©cran 1](images/Pasted%20image%2020251219200651.png)
![Capture d'√©cran 2](images/Pasted%20image%2020251219200820.png)

Une fois termin√©, on peut se rentre  sur l'IP qu'on a address√© sur la configuration du proxmox pendant son installation :

![Capture d'√©cran](images/Pasted%20image%2020251219200856.png)

Une fois Proxmox op√©rationnel, j'ai d√©ploy√© 3 VMs qui serviront de n≈ìuds pour notre futur cluster Ceph.


**Optimisation et Choix Techniques Ceph :** La configuration √† 3 n≈ìuds permet de tol√©rer la perte d'un n≈ìud sans interruption de service (N+1 redondance). L'algorithme CRUSH de Ceph assure une distribution pseudo-al√©atoire mais d√©terministe des donn√©es, garantissant un √©quilibrage de charge automatique. _Note :_ Dans un environnement de production critique, il est recommand√© de s√©parer physiquement le r√©seau public Ceph (acc√®s VMs) du r√©seau de cluster (r√©plication) pour √©viter les goulots d'√©tranglement lors des op√©rations de rebalancing. Dans notre architecture labo, ces flux cohabitent sur l'interface `eno1` via une segmentation logique

**Tableau des VMs Proxmox (Nested) :**

| **ID VM** | **Nom** | **OS**     | **vCPU** | **RAM** | **R√¥le** | IP           |
| --------- | ------- | ---------- | -------- | ------- | -------- | ------------ |
| 100       | PVE1    | Proxmox VE | 8 (Host) | 32 Go   | N≈ìud 1   | 10.202.6.220 |
| 101       | PVE2    | Proxmox VE | 8 (Host) | 32 Go   | N≈ìud 2   | 10.202.6.221 |
| 102       | PVE3    | Proxmox VE | 8 (Host) | 32 Go   | N≈ìud 3   | 10.202.6.222 |

---

## Jour 2 : Installation Hyper-V et Cluster Proxmox HA

### 1. Installation de Windows Server et Hyper-V (Romain)

De mon c√¥t√©, j'ai install√© la solution Microsoft sur le serveur 6.
#### Difficult√©s rencontr√©es

1. **Stockage :** N√©cessit√© de nettoyer les anciennes partitions des groupes pr√©c√©dents.
2. **Partitionnement :** Les disques avaient un formatage bloquant l'installateur Windows.
3. **ISO :** Incompatibilit√© de la premi√®re ISO test√©e.

Une fois Windows Server install√©, j'ai configur√© les **vSwitchs**. Apr√®s une tentative en mode Externe (Bridge) causant des probl√®mes APIPA, j'ai bascul√© vers un vSwitch Interne (NAT) pour stabiliser le r√©seau.

### 2. Mise en place de la Haute Disponibilit√© sur Proxmox (Alexandre)

Pendant ce temps, sur le serveur Proxmox, j'ai finalis√© la configuration du cluster pour la **Haute Disponibilit√© (HA)**.

- **Technologie utilis√©e :** J'ai coupl√© **Ceph (RBD)** pour le stockage distribu√© et **Corosync** pour la gestion du cluster.
- **Objectif :** Si le n≈ìud virtuel `pve1` tombe, les services doivent red√©marrer automatiquement sur `pve2` ou `pve3`.
- **R√©alisation :** Le cluster a √©t√© initialis√© via l'interface web et les premi√®res VMs de test (Alpine Linux) ont √©t√© d√©ploy√©es sur le pool de stockage partag√© `pool-vms`.
- 
J'ai configur√© les Vms Alpines, Alpine est une vm qui est extr√™mement l√©g√®re, et quad on boot dessus c'est un ISO live, donc pas de configuration tres longue, il faut juste faire un **setup-alpine** suivre le d√©roulement, setup de clavier, setup de l‚Äôadressage IP.... 
Une configuration rapide, pour une petite VM, est tr√®s suffisante pour le cadre de la SAE.
La mise en place de la haute disponibilit√© √©tait un succ√®s, comme nous pouvons le remarquer sur ces deux captures d'√©cran,


![Capture d'√©cran](images/Pasted%20image%2020251219230452.png)

La Vm 101 est donc r√©pliqu√©e dans le node 2

![Capture d'√©cran](images/Pasted%20image%2020251219230541.png)



Toutes les VMs proxmox en nested √©taient bridg√©es de l'eno1 vers le VMBR0 comme le montre cette capture d'√©cran

![Capture d'√©cran](images/Pasted%20image%2020251219221054.png)

---

## Jour 3 : R√©seau Hyper-V et √âchec PBS

### 1. R√©solution des probl√®mes r√©seaux Hyper-V (Romain)

J'ai pass√© une heure √† debuguer une erreur **"Destination Host Unreachable"** sur ses VMs.

- **Cause :** Conflit d'IP entre le vSwitch et la carte physique, et confusion dans le nommage des interfaces.
- **Solution :** R√©initialisation de la couche r√©seau et renommage propre des adaptateurs. Tout est rentr√© dans l'ordre.

Par la suite, j'ai mis en place un **Storage Space (Espace de stockage)** en mode miroir sur Windows avec 2 SSD du serveur pour cr√©er un disque **(J:)** s√©curis√©.

### 2. Commencement des livrables (Romain)

Apr√®s avoir r√©gl√© mes probl√®mes, j'ai d√©cid√© de prendre une pause sur les serveurs et consacrer du temps sur la r√©alisation des livrables (Compte rendu journalier et Bilan financier).

### 3. Tentative d'int√©gration Proxmox Backup Server (Alexandre)

J'ai tent√© d'int√©grer **Proxmox Backup Server (PBS)** pour g√©rer les sauvegardes.

- **Probl√®me critique :** L'importation de l'ISO dans le stockage local des n≈ìuds virtuels (Nested) provoquait un red√©marrage brutal (Kernel Panic) de l'hyperviseur parent.
- **Action :** Malgr√© une apr√®s-midi de tests avec M. Toulliou, le bug a persist√©. J'ai pris la d√©cision de pr√©parer une r√©installation compl√®te pour le lendemain afin de repartir sur des bases saines.
Un nouveau cahier des charges a √©t√© commenc√© ce jour l√† mais toujours dans l'esp√©rance que l'importation de l'ISO du PBS allait fonctionner

---

## Jour 4 : Diagnostic et Changement d'Architecture

### 1. Refonte de l'Architecture Proxmox (Alexandre)

Apr√®s de nouveaux tests infructueux avec M. Pouchoulon dans la matin√©e, j'ai valid√© la strat√©gie de refonte totale de la partie Proxmox pour adopter une approche **Hybride** :

- **Ceph (RBD) :** Maintenu pour les VMs critiques (Web, DNS) afin de garder la HA.
- **ZFS Local :** Introduit pour la VM "Client" pour gagner en performance disque.
- **ZFS R√©plication :** Pr√©vu pour le serveur de sauvegarde (PBS) afin d'assurer un Plan de Reprise d'Activit√© (PRA) sans d√©pendre du cluster Ceph.
- Cr√©ation d'un raid level 1 pour r√©plication sur les deux disques 1.7TO, comme cela, il n'y aura pas de disque inutilis√©s
Voici le sch√©ma de ma nouvelle construction :

Sch√©ma physique :

![Sch√©ma physique](images/Pasted%20image%2020251219204922.png)

Sch√©ma r√©seau :

![Sch√©ma r√©seau](images/Pasted%20image%2020251219205033.png)

### 2. Pr√©paration du Cluster S2D Hyper-V (Romain)

J'ai profit√© de cette demi-journ√©e pour pr√©parer le terrain pour le **Cluster S2D (Storage Spaces Direct)** : validation des pr√©requis r√©seaux, documentation des IPs et de mon sch√©ma r√©seau : 

![Sch√©ma r√©seau](images/imagex.png)

---

## Jour 4.5 (Romain/Week-end) : Cluster S2D √† distance

### Travail √† distance (VPN Tailscale)

Pour travailler √† distance pendant mon week-end, j'ai install√© le logiciel **Tailscale** sur mon serveur Windows. Une fois authentifi√©, j'ai pu prendre le contr√¥le total du bureau de mon serveur √† distance via RDP.

### Impl√©mentation du Cluster S2D

- **Mise √† niveau :** Passage de Windows Server Standard √† **Datacenter** (via cl√© KMS) pour d√©bloquer la fonction S2D.
- **Stockage :** Cr√©ation d'un pool de stockage partag√© utilisant les disques locaux des 3 n≈ìuds (Miroir √† 3 voies, ReFS).
- Test de r√©silience :
    
Pour faire ce test, nous regardons d'abord sur quel n≈ìud se trouve notre VM.
  
![Test de r√©silience](images/Pasted%20image%2020251219205153.png)

√âtant sur le n≈ìud 2, nous l'√©teignons pour voir comment r√©agit l'installation.

![Capture d'√©cran](images/Pasted%20image%2020251219205212.png)

R√©sultat : Le cluster d√©tecte automatiquement que le n≈ìud est tomb√©. Apr√®s quelques secondes, la VM a red√©marr√© sur le n≈ìud 1 (SRV1).

![Capture d'√©cran](images/Pasted%20image%2020251219205231.png)
![Sch√©ma r√©seau](images/image13.png)

-**Migration √† chaud :**¬†On active le "MAC Address Spoofing" pour permettre le d√©placement des VMs sans coupure r√©seau, puis, pour r√©aliser ce test, rienn de plus simple. On prend notre VM, et on appuie sur move -> live migration -> best possible node. Avant cela, on lance notre VM avec un ping infini vers google. Puis, on lance la migration dynamique. On voit qu'un ping est √† 12ms (au lieu de 6 ou 5 pour tout les autres) quand on clique sur la migration, mais ils reprennent de mani√®re normale juste apr√®s, sans coupure :

![Capture d'√©cran](images/Pasted%20image%2020251219205246.png)
![Capture d'√©cran](images/Pasted%20image%2020251219205312.png)
    
R√©sultat : Un ping l√©g√®rement plus haut (12ms) pendant la bascule, mais aucune coupure r√©seau.

---

## Jour 5 : La recofiguration enti√®re de Proxmox et derni√®re retouche Hyper-V

### 1. Refonte totale de la partie Proxmox (Alexandre)

Ayant valid√© la nouvelle architecture th√©orique, j'ai proc√©d√© √† la r√©installation compl√®te de l'environnement Proxmox avec un temps assez limit√© :

1. **Configuration iDRAC :** Cr√©ation du Virtual Disk en **RAID 1 (Miroir)** sur les SSD de 1.7 To pour la redondance physique.

 **![Capture d'√©cran](images/Pasted%20image%2020251219205447.png)

2. **D√©ploiement :** R√©installation de l'h√¥te et cr√©ation des 3 n≈ìuds virtuels, toujours les m√™mes.

3. **Stockage :** Configuration imm√©diate des pools Ceph et ZFS, et ZFS raid.
J'ai donc cr√©√© chaque 3 neouds virtuels avec 4 disques, un pour l'OS, 32 Go, et 100Go chacun pour un ZFS, ZFS raid et pour le CEPH, j'ai donc une architecture de ce type l√†

### 2. Peaufinage de Hyper-V (Romain)

J'ai pris un peu de temps pour m'assurer du fait que la migration dynamique se faisait bien de tous les serveurs vers les autres pour valider le cluster √† basculement.

### 3. Continuit√© des livrables (Commun)

Pendant qu'Alexandre faisait sa partie Proxmox plus longue que pr√©vue, √† cause du crash, Romain a avanc√© le comparatif financier. Alexandre a ensuite document√© sa refonte.

---

## Jour 6 : D√©ploiement des Services et Dimensionnement

### Alexandre :

J'ai finalis√© l'infrastructure Proxmox en d√©ployant les services finaux. J'ai choisi **Alpine Linux** pour sa l√©g√®ret√©.

### 1. D√©ploiement et Configuration D√©taill√©e des Services (VMs)

Pour cette √©tape, j'ai fait le choix technique d'utiliser Alpine Linux.

Pourquoi ? C'est une distribution orient√©e s√©curit√© et ultra-l√©g√®re. Une VM Alpine consomme environ 50 Mo de RAM au repos, ce qui est crucial √©tant donn√© que nos ressources RAM sont limit√©es sur le cluster virtuel.

Configuration standard des VMs : **2 vCores, 512 Mo RAM, 8 Go Disque**.

#### A. VM 105 : Le Serveur DNS (Bind9)

**IP :** `10.202.6.53` | **Stockage :** Ceph RBD (Haute Disponibilit√©)

Le r√¥le de cette VM est de traduire les noms de domaine (`www.sae.lan`) en adresses IP. J'ai choisi **ISC Bind9**, le standard de l'industrie.

1. Installation et Configuration R√©seau :

Apr√®s avoir install√© l'OS, j'ai fix√© l'IP statique dans /etc/network/interfaces puis install√© le paquet :

```
apk add bind
```

2. D√©claration de la Zone (named.conf.local) :

J'ai configur√© Bind pour qu'il soit "Ma√Ætre" de la zone sae.lan. J'ai √©dit√© le fichier de configuration pour d√©clarer cette nouvelle zone :

```
zone "sae.lan" {
    type master;
    file "/etc/bind/db.sae.lan";
};
```

3. Cr√©ation du Fichier de Zone (db.sae.lan) :

C'est l'√©tape la plus technique. J'ai cr√©√© le fichier de base de donn√©es DNS contenant les enregistrements (Records).

- **SOA (Start of Authority) :** D√©finit les param√®tres globaux (TTL, Email admin).
- **NS (Name Server) :** Indique qui est le serveur DNS (lui-m√™me).
- **A (Address) :** Fait le lien IP ‚Üî Nom.

Voici la configuration exacte que j'ai inject√©e :

```
$TTL 604800
@       IN      SOA     ns1.sae.lan. admin.sae.lan. (
                              2         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
; Serveur de Noms
@       IN      NS      ns1.sae.lan.

; Enregistrements A (Nos services)
@       IN      A       10.202.6.53
ns1     IN      A       10.202.6.53
www     IN      A       10.202.6.54     ; Pointeur vers le Web
pbs     IN      A       10.202.6.255    ; Pointeur vers le Backup
```

Une fois configur√©, j'ai activ√© le service au d√©marrage : `rc-update add named default && rc-service named start`.

#### B. VM 106 : Le Serveur Web (Nginx)

**IP :** `10.202.6.54` | **Stockage :** Ceph RBD (Haute Disponibilit√©)

Pour le serveur Web, j'ai opt√© pour **Nginx** plut√¥t qu'Apache. Nginx est r√©put√© pour sa capacit√© √† g√©rer beaucoup de connexions simultan√©es avec tr√®s peu de m√©moire, ce qui s'aligne avec notre philosophie "Alpine".

**1. Installation :**

```
apk add nginx
```

2. Configuration du Site :

J'ai modifi√© le fichier /var/lib/nginx/html/index.html pour cr√©er une page d'accueil personnalis√©e prouvant que le serveur r√©pond bien.

J'ai ensuite v√©rifi√© la configuration du vHost dans /etc/nginx/http.d/default.conf pour m'assurer qu'il √©coutait bien sur le port 80.

3. Test local :

Un curl localhost sur la VM m'a confirm√© que le serveur web tournait correctement avant m√™me de tester depuis le client.

#### C. VM 107 : Le Client de Test

**IP :** DHCP | **Stockage :** ZFS Local

Cette machine simule un utilisateur du r√©seau.

- **Particularit√© Stockage :** J'ai plac√© son disque sur le stockage **ZFS Local** (et non Ceph). Comme c'est une machine de test jetable, elle n'a pas besoin de Haute Disponibilit√©, mais elle b√©n√©ficie ainsi de meilleures performances disques (I/O) pour lancer des benchmarks si besoin.
- **Outils install√©s :** `bind-tools` (pour avoir `nslookup` et `dig`) et `curl` (pour tester le web).

![Capture d'√©cran](images/Pasted%20image%2020251219205818.png)

### 2. Le Serveur de Sauvegarde (PBS)

Installation de **Proxmox Backup Server** (VM 110) sur un stockage **ZFS R√©pliqu√©** (pour la s√©curit√©).
- **Ressources :** 4 Go RAM (pour la d√©duplication), 20 Go Disque.
- **IP :** `10.202.6.255`
- **Liaison :** Le Datacenter PVE a √©t√© connect√© au PBS pour permettre les backups.
- 
![Capture d'√©cran](images/Pasted%20image%2020251219212942.png)

### Romain :

Ayant fini ma partie technique, j'ai pu me consacrer √† 100% aux livrables, j'ai mis au propre et en markdown ce compte rendu. J'ai ensuite mis au propre mes id√©es par rapport aux bilans financiers sur Excel.

---

## Jour 7 : Supervision et Tests Finaux

### 1. Supervision Centralis√©e "Datacenter Manager" (Alexandre)

Pour assurer la veille technologique et la gestion globale, j'ai install√© une solution de gestion centralis√©e (Datacenter Manager) sur une VM d√©di√©e. Cela nous permet de visualiser la charge du cluster en temps r√©el via une interface unifi√©e.

![Capture d'√©cran](images/Pasted%20image%2020251219213027.png)

### 2. Validation Fonctionnelle (Alexandre)

Nous avons cl√¥tur√© la semaine par les tests de validation :

- Test Service : Le client acc√®de bien au site web www.sae.lan via le DNS interne.
    
![Capture d'√©cran](images/Pasted%20image%2020251219205934.png)

La page de base du localhost d'un service Nginx correspond exactement a ce fichier HTML, c'est une r√©ussite, cela veut dire que mon serveur nginx et mon service DNS sont tous les deux op√©rationnels


### 3. Bilan Financier et Documentation (Commun)

En cette fin de projet, nous avons mis en commun nos rapports et nos exp√©riences. Bien que nous faisions des choses assez proches, le fait que l'un soit sur Proxmox et l'autre sur Hyper-V a enrichi notre vision globale.

---

# PARTIE 4 : BILAN FINANCIER & ANALYSE DE RENTABILIT√â 

## 1. Introduction et Param√®tres de l'√âtude

Afin de d√©partager les solutions **Proxmox VE** et **Microsoft Hyper-V**, nous ne nous sommes pas limit√©s √† la technique. Nous avons r√©alis√© une projection financi√®re sur **6 ans (72 mois)** pour une infrastructure type compos√©e de **3 n≈ìuds physiques**.

L'indicateur cl√© utilis√© est le **TCO (Total Cost of Ownership)**, qui englobe le co√ªt d'achat (CAPEX) et le co√ªt de fonctionnement (OPEX).

### Hypoth√®ses de calcul :

- **Infrastructure :** 3 Serveurs Physiques (N√©cessaire pour le quorum Ceph/S2D).
- **Charge de travail :** Sc√©nario de base avec **15 Machines Virtuelles** (Windows Server).
- **Co√ªts Humains :** Salaire administrateur charg√© √† 45‚Ç¨/h.
- **√ânergie :** 0,25‚Ç¨/kWh.

---

## 2. Investissement Initial (CAPEX)

Ce tableau repr√©sente la tr√©sorerie √† sortir "Jour 1" pour monter l'infrastructure.

|**Postes de Co√ªts (Achat)**|**PROXMOX (Cluster Ceph)**|**HYPER-V (S2D)**|**Analyse & Explication**|
|---|---|---|---|
|**Mat√©riel (3 Serveurs)**|30 000 ‚Ç¨|34 000 ‚Ç¨|Hyper-V n√©cessite souvent du mat√©riel certifi√©/valid√© (HCL) plus strict et cher.|
|**R√©seau (Switchs 10/25GbE)**|4 000 ‚Ç¨|4 000 ‚Ç¨|Idem pour les deux : le stockage distribu√© (Ceph ou S2D) exige du r√©seau rapide.|
|**Licences OS (H√¥tes)**|**0 ‚Ç¨**|**18 900 ‚Ç¨**|**L'√©cart majeur.** Proxmox est gratuit. Hyper-V n√©cessite 3 licences "Datacenter" (6300‚Ç¨/CPU).|
|**Logiciel de Sauvegarde**|0 ‚Ç¨|5 000 ‚Ç¨|Proxmox Backup Server est gratuit. Hyper-V requiert souvent Veeam (payant).|
|**Prestation d'Installation**|3 000 ‚Ç¨|2 000 ‚Ç¨|Ceph demande une configuration plus fine au d√©marrage que l'assistant S2D Microsoft.|
|**Formation √âquipe**|1 500 ‚Ç¨|0 ‚Ç¨|Budget provisionn√© pour la mont√©e en comp√©tence Linux de l'√©quipe (si n√©cessaire).|
|**TOTAL CAPEX**|**38 500 ‚Ç¨**|**63 900 ‚Ç¨**|üî¥ **Hyper-V est 65% plus cher au d√©marrage.**|

---

## 3. Co√ªts de Fonctionnement (OPEX)

Ce tableau repr√©sente ce que l'infrastructure co√ªte chaque jour pendant 6 ans (Maintenance, √âlectricit√©, Support).

|**Postes de Co√ªts (sur 6 ans)**|**PROXMOX**|**HYPER-V**|**Analyse & Explication**|
|---|---|---|---|
|**Support / Maintenance**|1 350 ‚Ç¨|3 000 ‚Ç¨|Comparatif entre l'abonnement "Community" Proxmox et la Software Assurance Microsoft.|
|**√âlectricit√©**|2 700 ‚Ç¨|3 000 ‚Ç¨|Windows Server (avec interface graphique) consomme l√©g√®rement plus de ressources "√† vide".|
|**Main d'≈ìuvre (Maintenance)**|1 800 ‚Ç¨|2 700 ‚Ç¨|**Le temps homme.** Les mises √† jour de clusters Windows (S2D) sont lourdes, lentes et demandent plus de reboots que Linux.|
|**TOTAL OPEX (6 ANS)**|**35 100 ‚Ç¨**|**52 200 ‚Ç¨**|üü¢ **Proxmox co√ªte moins cher √† maintenir dans le temps.**|

---

## 4. Verdict Final : Le Co√ªt Total (TCO)

C'est ici que se joue la d√©cision finale. Nous comparons le co√ªt global pour h√©berger nos **15 VMs**.

- **Strat√©gie Hyper-V :** On paie tr√®s cher les h√¥tes physiques (Licence Datacenter), mais cela donne le droit de cr√©er des VMs Windows **illimit√©es** gratuitement.
- **Strat√©gie Proxmox :** L'h√¥te est gratuit, mais on doit acheter une licence Windows Standard (750‚Ç¨) pour **chaque VM** cr√©√©e.

### Bilan pour 15 VMs :

|**Type de Co√ªt**|**PROXMOX**|**HYPER-V**|**Notes**|
|---|---|---|---|
|**Investissement (Capex)**|38 500 ‚Ç¨|63 900 ‚Ç¨||
|**Fonctionnement (Opex)**|35 100 ‚Ç¨|52 200 ‚Ç¨||
|**Co√ªt Licences VMs (15 VMs)**|**11 250 ‚Ç¨**|**Inclus (0‚Ç¨)**|15 x 750‚Ç¨ pour Proxmox. Gratuit pour Hyper-V (car Datacenter).|
|**CO√õT TOTAL (6 ANS)**|**84 850 ‚Ç¨**|**116 100 ‚Ç¨**||
|**√âCONOMIE R√âALIS√âE**|üü¢ **31 250 ‚Ç¨**||**Proxmox est nettement plus rentable.**|

---

## 5. Analyse de Sensibilit√© : Quand Hyper-V devient-il rentable ?

Une question subsiste : _√Ä partir de combien de VMs la licence forfaitaire "Datacenter" de Microsoft devient-elle plus int√©ressante que l'achat unitaire de licences sur Proxmox ?_

Nous avons mod√©lis√© l'√©volution des co√ªts en fonction de la densit√© de VMs :

|**SC√âNARIO (Nombre de VMs)**|**CO√õT TOTAL PROXMOX**|**CO√õT TOTAL HYPER-V**|**R√âSULTAT**|
|---|---|---|---|
|**3 VMs** (Infrastructure minime)|75 850 ‚Ç¨|116 100 ‚Ç¨|üü¢ Proxmox beaucoup moins cher.|
|**15 VMs** (Notre cas)|84 850 ‚Ç¨|116 100 ‚Ç¨|üü¢ Toujours gagnant (Gain : ~31k‚Ç¨).|
|**40 VMs** (Forte densit√©)|103 600 ‚Ç¨|116 100 ‚Ç¨|üü¢ Toujours gagnant (Gain : ~12k‚Ç¨).|
|**58 VMs** (**Point de bascule**)|**117 100 ‚Ç¨**|**116 100 ‚Ç¨**|üî¥ **Hyper-V devient plus rentable ici.**|

### Conclusion Financi√®re

Tant que l'infrastructure h√©berge **moins de 58 VMs Windows**, la solution **Proxmox VE** est √©conomiquement sup√©rieure. Dans notre cas d'√©cole (15 √† 30 VMs), l'√©conomie se chiffre en dizaines de milliers d'euros, ce qui valide d√©finitivement le choix de l'Open Source.
